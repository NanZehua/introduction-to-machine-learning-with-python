# 无监督学习的类型与预处理

训练算法时只有输入数据，没有已知输出。

## 无监督学习类型

### 无监督变换

数据集的无监督变换，变换后的特征可能更容易被算法理解。

也常用于把高维特征的数据降维为较少特征的数据，带来一个好处就是降维后就能够可视化分析。

### 聚类算法

该类算法能够把输入数据划分成不同的组，每组包含类似的物项。

## 无监督学习的挑战

因为训练数据没有分类标签，所以很难用精度去评估模型有效。

因此，无监督算法通常用于探索数据的规律，而不是自动化系统。

无监督算法的另外一个用途是作为监督算法的预处理步骤，之前提到过可以进行特征变换，有时可以提高监督算法的精度、或者减少内存以及计算开销。

## 预处理与缩放

之前用过的SVM、神经网络对特征缩放非常敏感，需要对原始数据进行预处理。


```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# 数据集
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

# 先仅仅训练特征缩放器，只需要数据特征, 不需要标签
scaler = MinMaxScaler()
scaler.fit(X_train)

# 然后对训练集的特征完成实际缩放
X_train_scaled = scaler.transform(X_train)

# 打印缩放前和缩放后，训练集中每种特征的最小值和最大值
print("transformed shape: {}".format(X_train_scaled.shape)) 
print("per-feature minimum before scaling:\n {}".format(X_train.min(axis=0))) 
print("per-feature maximum before scaling:\n {}".format(X_train.max(axis=0))) 
print("per-feature minimum after scaling:\n {}".format(X_train_scaled.min(axis=0)))
print("per-feature maximum after scaling:\n {}".format(X_train_scaled.max(axis=0)))
```

```
transformed shape: (426, 30)
per-feature minimum before scaling:
 [6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00
 0.000e+00 1.060e-01 5.024e-02 1.153e-01 3.602e-01 7.570e-01 6.802e+00
 1.713e-03 2.252e-03 0.000e+00 0.000e+00 9.539e-03 8.948e-04 7.930e+00
 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00
 1.566e-01 5.521e-02]
per-feature maximum before scaling:
 [2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 2.867e-01 4.268e-01
 2.012e-01 3.040e-01 9.575e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02
 3.113e-02 1.354e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01
 4.954e+01 2.512e+02 4.254e+03 2.226e-01 9.379e-01 1.170e+00 2.910e-01
 5.774e-01 1.486e-01]
per-feature minimum after scaling:
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0.]
per-feature maximum after scaling:
 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1.]
```

即经过MinMaxScaler处理后，所有特征的值都缩放到0~1之间。

变换后的数据形状（特征之间的比例）与原始数据相同，特征只是发生了移动与缩放。

对测试集进行相同的缩放，也就是用训练集fit好的scaler在测试集执行，不必担心测试集的特征值范围和训练集不同导致什么问题，保持一致的缩放比例才是正确的。

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# 数据集
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

# 先仅仅训练特征缩放器，只需要数据特征, 不需要标签
scaler = MinMaxScaler()
scaler.fit(X_train)

# 然后对训练集的特征完成实际缩放
X_train_scaled = scaler.transform(X_train)

# 对测试数据进行变换
X_test_scaled = scaler.transform(X_test)
# 在缩放之后打印测试数据的属性
print("per-feature minimum after scaling:\n{}".format(X_test_scaled.min(axis=0))) 
print("per-feature maximum after scaling:\n{}".format(X_test_scaled.max(axis=0)))
```

```
per-feature minimum after scaling:
[ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704
  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501
  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.
 -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767
  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]
per-feature maximum after scaling:
[0.9578778  0.81501522 0.95577362 0.89353128 0.81132075 1.21958701
 0.87956888 0.9333996  0.93232323 1.0371347  0.42669616 0.49765736
 0.44117231 0.28371044 0.48703131 0.73863671 0.76717172 0.62928585
 1.33685792 0.39057253 0.89612238 0.79317697 0.84859804 0.74488793
 0.9154725  1.13188961 1.07008547 0.92371134 1.20532319 1.63068851]
```

先在原始数据上训练SVC模型，看一下精度：

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# 数据集
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

# 训练SVC分类器
svm = SVC(C=100)
svm.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test, y_test)))
```

```
Test set accuracy: 0.62
```

测试集精度只有0.62。

再试试缩放后的特征作为输入。

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# 数据集
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

# 先仅仅训练特征缩放器，只需要数据特征, 不需要标签
scaler = MinMaxScaler()
scaler.fit(X_train)

# 然后对训练集的特征完成实际缩放
X_train_scaled = scaler.transform(X_train)
# 对测试数据进行变换
X_test_scaled = scaler.transform(X_test)

# 训练SVC
svm = SVC(C=100)
svm.fit(X_train_scaled, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test_scaled, y_test)))
```

```
Test set accuracy: 0.97
```

精度高达0.97。

也可以尝试其他的预处理缩放算法替换MinMaxScaler，比如：StandardScaler。

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 数据集
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)

# 先仅仅训练特征缩放器，只需要数据特征, 不需要标签
scaler = StandardScaler()
scaler.fit(X_train)

# 然后对训练集的特征完成实际缩放
X_train_scaled = scaler.transform(X_train)
# 对测试数据进行变换
X_test_scaled = scaler.transform(X_test)

# 训练SVC
svm = SVC(C=100)
svm.fit(X_train_scaled, y_train)
print("Test set accuracy: {:.2f}".format(svm.score(X_test_scaled, y_test)))

```

```
Test set accuracy: 0.97
```

精度也很棒。

# 降维、特征提取与流行学习

## 主成分分析

PCA可以从已有特征产生出新特征，新特征对解释数据更好用。

PCA可以指定提取的特征个数，实现降维的目的。

### 可视化用途

下面对cancer数据集利用PCA从30个维降到2个维度

```
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

#  数据集
cancer = load_breast_cancer()

# 标准化
scaler = StandardScaler()
scaler.fit(cancer.data)
X_scaled = scaler.transform(cancer.data)

# 降维到2的PCA
pca = PCA(n_components=2)
# 训练PCA
pca.fit(X_scaled)
# 进行特征提取变换
X_pca = pca.transform(X_scaled)
# 降维前的特征个数
print("Original shape: {}".format(str(X_scaled.shape))) 
# 降维后的特征个数
print("Reduced shape: {}".format(str(X_pca.shape)))
```

```
Original shape: (569, 30)
Reduced shape: (569, 2)
```

因为PCA后只有2个特征维度，所以可以绘图做可视化分析。

### 主成分提取用途

图像分类场景，因为图片特征都是像素点，只有把所有像素整体看待才有意义，所以图像特征经过PCA处理后精度会更好。

```
from sklearn.datasets import fetch_lfw_people
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

# 加载数据
people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)

# 3023张人脸照片作为样本输入
print("people.images.shape: {}".format(people.images.shape))
# 3023张图片对应的人名
print("Number of classes: {}".format(len(people.target_names)))

# 3023列的false向量
mask = np.zeros(people.target.shape, dtype=np.bool)

# 遍历所有的分类(也就是人名，target取值是0~61)
# 每个分类保留50个样本
for target in np.unique(people.target):
    mask[np.where(people.target == target)[0][:50]] = True
X_people = people.data[mask]  # data是已经把每张图片n*m的2维压成1维的数据格式
y_people = people.target[mask]

# 现在留下的样本，每个人不会超过50张

# 特征缩放到0~1之间, 因为是RGB颜色
X_people = X_people / 255

# 切分数据
#print(X_people, y_people)
X_train, X_test, y_train, y_test = train_test_split(
    X_people, y_people, stratify=y_people, random_state=0)

# KNN分类, 最近邻分类(只考虑最近的节点)
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

print(knn.score(X_test, y_test))
```

```
0.23255813953488372
```

精度不高，尝试通过PCA提取100个主成分作为新特征。

```
from sklearn.datasets import fetch_lfw_people
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

# 加载数据
people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)

# 3023张人脸照片作为样本输入
print("people.images.shape: {}".format(people.images.shape))
# 3023张图片对应的人名
print("Number of classes: {}".format(len(people.target_names)))

# 3023列的false向量
mask = np.zeros(people.target.shape, dtype=np.bool)

# 遍历所有的分类(也就是人名，target取值是0~61)
# 每个分类保留50个样本
for target in np.unique(people.target):
    mask[np.where(people.target == target)[0][:50]] = True
X_people = people.data[mask]  # data是已经把每张图片n*m的2维压成1维的数据格式
y_people = people.target[mask]

# 现在留下的样本，每个人不会超过50张

# 特征缩放到0~1之间, 因为是RGB颜色
X_people = X_people / 255

# 切分数据
#print(X_people, y_people)
X_train, X_test, y_train, y_test = train_test_split(
    X_people, y_people, stratify=y_people, random_state=0)

# PCA提取100个主成分作为新的特征, 基于训练集fit, 应用到训练集和测试集
pca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
print("X_train_pca.shape: {}".format(X_train_pca.shape))

# KNN分类, 最近邻分类(只考虑最近的节点)
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train_pca, y_train)

# 精度
print(knn.score(X_test_pca, y_test))
```

```
0.312015503875969
```

PCA通过提取主成分作为新特征带来了精度提升。

