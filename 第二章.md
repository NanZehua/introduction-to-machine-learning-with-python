# 分类与回归

分类是预测标签，包括二分类与多分类。

回归是预测连续值，比如预测收入、房价。

# 泛化、过拟合与欠拟合

随着模型算法逐渐复杂，其在训练集上的预测精度将提高，但在测试集上的预测精度将降低，因此模型的复杂度需要折衷。

模型过于复杂，将导致模型泛化能力差，即过拟合。
模型过于简单，将导致模型精度在训练集表现就很差，更不用说测试集的表现了，此时即欠拟合。

## 模型复杂度与数据集大小的关系

数据点的值变化范围越大，则可以应用更加复杂的模型，预测的表现也会越好。

更多的训练数据往往伴随着更大范围的特征值变化，因此可以应用更复杂的模型算法。

但注意，如果是非常类似的数据点，无论数据集多大也是无济于事的。

# 样本数据说明

## 2个低维度数据集

这两个数据集很小，特征维度很低，不超过2维。

用于分类的forge数据集，2个特征输入。

```
import mglearn
import matplotlib.pyplot as plt

# 生成forge样本的特征X和目标y
X, y = mglearn.datasets.make_forge()

# 使用样本的第0列特征和第1列特征作为绘制的横坐标和纵坐标，目标y作为图案
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
# 在右下角画一个图案的文字说明，即2个分类
plt.legend(["Class 0", "Class 1"], loc=4) 
# 绘制横坐标的说明
plt.xlabel("First feature")
# 绘制纵坐标的说明
plt.ylabel("Second feature")
# 样本的个数和特征的维度
print("X.shape: {}".format(X.shape))
```

用于回归的wave数据集，1个特征输入。

```
import mglearn
import matplotlib.pyplot as plt

#构造40个样本
X, y = mglearn.datasets.make_wave(n_samples=40)
#因为X只有1维, 所以直接可以画散点图
plt.plot(X, y, 'o')
#y的连续值范围
plt.ylim(-3, 3)
# 画横坐标说明
plt.xlabel("Feature")
# 画纵坐标说明
plt.ylabel("Target")
```

## 2个高维度数据集

用于分类的cancer癌症数据集，569个样本，30维特征。

```
from sklearn.datasets import load_breast_cancer
import numpy as np

# 加载数据集
cancer = load_breast_cancer()
# 打印样本规模和特征规模
print(cancer.data.shape)
# 打印不同分类的样本数量, np.bincount统计不同分类的个数, 然后与分类的名字做1:1 zip，得到每个分类的样本数量
print("Sample counts per class:\n{}".format(
{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))
```

```
(569, 30)
Sample counts per class:
{'malignant': 212, 'benign': 357}
```


用于回归的boston房价数据集。

```
from sklearn.datasets import load_boston
boston = load_boston()
print("Data shape: {}".format(boston.data.shape))
```

以及对原有特征经过简单的"特征工程"，增加了若干组合特征，得到的extened_boston房价数据集：

```
from sklearn.datasets import load_boston
X, y = mglearn.datasets.load_extended_boston()
print("X.shape: {}".format(X.shape))
```

# K邻近

分类forge数据集。

```
from sklearn.model_selection import train_test_split
import mglearn

X, y = mglearn.datasets.make_forge()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 计算最近3个点中最多出现的分类作为预测标签
clf = KNeighborsClassifier(n_neighbors=3)
clf.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(clf.score(X_test, y_test)))
```

分类cancer数据集。

```
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 加载数据
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, stratify=cancer.target, random_state=66)

# 记录不同n_neighbors情况下，模型的训练集精度与测试集精度的变化
training_accuracy = [] 
test_accuracy = []

# n_neighbors取值从1到10 
neighbors_settings = range(1, 11)
for n_neighbors in neighbors_settings:
    # 模型对象
    clf = KNeighborsClassifier(n_neighbors=n_neighbors) 
    # 训练
    clf.fit(X_train, y_train)
    # 记录训练集精度
    training_accuracy.append(clf.score(X_train, y_train)) 
    # 记录测试集精度
    test_accuracy.append(clf.score(X_test, y_test))

# 画出2条曲线，横坐标是邻居个数，纵坐标分别是训练集精度和测试集精度
plt.plot(neighbors_settings, training_accuracy, label="training accuracy")
plt.plot(neighbors_settings, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()
```

调大n_neighbors则导致训练集精度下降，测试集精度上升，折衷点在n_neighbors=6。



