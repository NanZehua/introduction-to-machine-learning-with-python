# 分类与回归

分类是预测标签，包括二分类与多分类。

回归是预测连续值，比如预测收入、房价。

# 泛化、过拟合与欠拟合

随着模型算法逐渐复杂，其在训练集上的预测精度将提高，但在测试集上的预测精度将降低，因此模型的复杂度需要折衷。

模型过于复杂，将导致模型泛化能力差，即过拟合。
模型过于简单，将导致模型精度在训练集表现就很差，更不用说测试集的表现了，此时即欠拟合。

## 模型复杂度与数据集大小的关系

数据点的值变化范围越大，则可以应用更加复杂的模型，预测的表现也会越好。

更多的训练数据往往伴随着更大范围的特征值变化，因此可以应用更复杂的模型算法。

但注意，如果是非常类似的数据点，无论数据集多大也是无济于事的。

# 样本数据说明

## 2个低维度数据集

这两个数据集很小，特征维度很低，不超过2维。

用于分类的forge数据集，2个特征输入。

```
import mglearn
import matplotlib.pyplot as plt

# 生成forge样本的特征X和目标y
X, y = mglearn.datasets.make_forge()

# 使用样本的第0列特征和第1列特征作为绘制的横坐标和纵坐标，目标y作为图案
mglearn.discrete_scatter(X[:, 0], X[:, 1], y)
# 在右下角画一个图案的文字说明，即2个分类
plt.legend(["Class 0", "Class 1"], loc=4) 
# 绘制横坐标的说明
plt.xlabel("First feature")
# 绘制纵坐标的说明
plt.ylabel("Second feature")
# 样本的个数和特征的维度
print("X.shape: {}".format(X.shape))
```

用于回归的wave数据集，1个特征输入。

```
import mglearn
import matplotlib.pyplot as plt

#构造40个样本
X, y = mglearn.datasets.make_wave(n_samples=40)
#因为X只有1维, 所以直接可以画散点图
plt.plot(X, y, 'o')
#y的连续值范围
plt.ylim(-3, 3)
# 画横坐标说明
plt.xlabel("Feature")
# 画纵坐标说明
plt.ylabel("Target")
```

## 2个高维度数据集

用于分类的cancer癌症数据集，569个样本，30维特征。

```
from sklearn.datasets import load_breast_cancer
import numpy as np

# 加载数据集
cancer = load_breast_cancer()
# 打印样本规模和特征规模
print(cancer.data.shape)
# 打印不同分类的样本数量, np.bincount统计不同分类的个数, 然后与分类的名字做1:1 zip，得到每个分类的样本数量
print("Sample counts per class:\n{}".format(
{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))
```

```
(569, 30)
Sample counts per class:
{'malignant': 212, 'benign': 357}
```


用于回归的boston房价数据集。

```
from sklearn.datasets import load_boston
boston = load_boston()
print("Data shape: {}".format(boston.data.shape))
```

以及对原有特征经过简单的"特征工程"，增加了若干组合特征，得到的extened_boston房价数据集：

```
from sklearn.datasets import load_boston
X, y = mglearn.datasets.load_extended_boston()
print("X.shape: {}".format(X.shape))
```

# K邻近

## 分类forge数据集

```
from sklearn.model_selection import train_test_split
import mglearn

X, y = mglearn.datasets.make_forge()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 计算最近3个点中最多出现的分类作为预测标签
clf = KNeighborsClassifier(n_neighbors=3)
clf.fit(X_train, y_train)
print("Test set accuracy: {:.2f}".format(clf.score(X_test, y_test)))
```

## 分类cancer数据集

```
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 加载数据
cancer = load_breast_cancer()

# 切分
X_train, X_test, y_train, y_test = train_test_split(
    cancer.data, cancer.target, stratify=cancer.target, random_state=66)

# 记录不同n_neighbors情况下，模型的训练集精度与测试集精度的变化
training_accuracy = [] 
test_accuracy = []

# n_neighbors取值从1到10 
neighbors_settings = range(1, 11)
for n_neighbors in neighbors_settings:
    # 模型对象
    clf = KNeighborsClassifier(n_neighbors=n_neighbors) 
    # 训练
    clf.fit(X_train, y_train)
    # 记录训练集精度
    training_accuracy.append(clf.score(X_train, y_train)) 
    # 记录测试集精度
    test_accuracy.append(clf.score(X_test, y_test))

# 画出2条曲线，横坐标是邻居个数，纵坐标分别是训练集精度和测试集精度
plt.plot(neighbors_settings, training_accuracy, label="training accuracy")
plt.plot(neighbors_settings, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()
```

调大n_neighbors则导致训练集精度下降，测试集精度上升，折衷点在n_neighbors=6，此时模型既不会过拟合也不会欠拟合，这就是调参。

## 回归wave数据集

```
from sklearn.neighbors import KNeighborsRegressor

# 加载数据集
X, y = mglearn.datasets.make_wave(n_samples=40)

# 将wave数据集分为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 模型实例化，并将邻居个数设为3
reg = KNeighborsRegressor(n_neighbors=3) 
# 利用训练数据和训练目标值来拟合模型 
reg.fit(X_train, y_train)
# predict测试集
print("Test set predictions:\n{}".format(reg.predict(X_test)))
# 评估模型
print("Test set R^2: {:.2f}".format(reg.score(X_test, y_test)))
```

调大n_neighbors具备更好的泛化，但是对训练集的预测精度下降。

## 结论

* knn模型2个重要参数：邻居个数与数据点之间的距离度量方式。
* 推荐选择3-5个邻居
* 在大数据集上处理慢，特征过多或者特征0值多均导致效果不佳



# 线性模型

对于回归问题，线性模型预测的一般公式是：
ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b

有许多种不同的线性回归模型，区别在于模型如何学习到参数w和b，以及如何控制模型复杂度。

## 线性回归

回归问题最简单最经典的线性模型。

它试图找到参数w和b，使得预测值和真实值之间的均方误差最小。

均方误差(mean squared error)是预测值与真实值之差的平方和除 以样本数。

### 回归wave

```
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import mglearn

# 生成60个样本数据, 一维特征
X, y = mglearn.datasets.make_wave(n_samples=60)
# 切分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
# 训练线性回归模型
lr = LinearRegression().fit(X_train, y_train)

# coef_就是斜率w, 即每个特征对应一个权重
print("lr.coef_: {}".format(lr.coef_))
# intercept_是截距b
print("lr.intercept_: {}".format(lr.intercept_))

# 训练集精度
print("Training set score: {:.2f}".format(lr.score(X_train, y_train)))
# 测试机精度
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))
```

```
Training set score: 0.67
Test set score: 0.66
```

效果不佳，说明模型过于简单，存在欠拟合。

换成更高维的数据集（有更多特征的），线性模型将表现不同。

### 回归extended_boston

```
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import mglearn

# 波士顿extended数据集
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
lr = LinearRegression().fit(X_train, y_train)

print("Training set score: {:.2f}".format(lr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))
```

```
Training set score: 0.95
Test set score: 0.61
```

可见"线性回归"表现出很严重的过拟合，一个表现更好的模型就是"岭回归"。

## 岭回归

采用线性回归同样的公式，但是模型约束学习得到的w系数尽可能的接近于0，即每个特征对输出的影响尽可能小，从而避免过拟合。

这个约束叫做正则化，岭回归用到的是L2正则化。

```
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import mglearn

# 波士顿extended数据集
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
ridge = Ridge().fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge.score(X_test, y_test)))

```

```
Training set score: 0.89
Test set score: 0.75
```

岭回归泛化能力优于线性回归，带来的就是训练集精度下降，测试集精度上升。

该模型支持alpha参数，该参数默认为1，调大alpha会进一步下降训练集精度，可能加强泛化能力；相反，调小alpha则减少了约束，训练集精度上升，可能降低泛化能力。

```
ridge10 = Ridge(alpha=10).fit(X_train, y_train)
print("Training set score: {:.2f}".format(ridge10.score(X_train, y_train)))
print("Test set score: {:.2f}".format(ridge10.score(X_test, y_test)))
```

```
Training set score: 0.79
Test set score: 0.64
```

在相同训练数据量下，经过正则化的模型在训练集上的精度偏低，非正则化的则泛化能力较差。

但是当训练集足够大的情况下，这种差别就不明显了，两种模型的测试集精度大致相当。

## lasso回归

与岭回归类似，采用了另外一种正则化叫做L1正则化，它可以约束某些w系数为0，相当于自动筛掉了一些没用的特征。

```
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import numpy as np
import mglearn

# 波士顿extended数据集
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
lasso = Lasso().fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso.score(X_test, y_test)))
# lasso.coef_是w斜率向量，数一下有几个特征的系数不为0
print("Number of features used: {}".format(np.sum(lasso.coef_ != 0)))

```

```
Training set score: 0.29
Test set score: 0.21
Number of features used: 4
```

该模型只用到了105个特征中的4个，其他的w系数都是0。

该模型预测精度很差，属于欠拟合，需要减少模型的alpha参数，即放松正则化L1。

```
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
import numpy as np
import mglearn

# 波士顿extended数据集
X, y = mglearn.datasets.load_extended_boston()

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    
# 我们增大max_iter的值，否则模型会警告我们，说应该增大max_iter
lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) 
print("Training set score: {:.2f}".format(lasso001.score(X_train, y_train))) 
print("Test set score: {:.2f}".format(lasso001.score(X_test, y_test))) 
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))
```

在放松正则化的同时，模型需要增加迭代的次数max_iter，这次用到了33个特征，模型精度上升。

```
Training set score: 0.90
Test set score: 0.77
Number of features used: 33
```

进一步调小alpha，将会令该模型等效于线性回归，产生过拟合。

### 小结

优先选岭回归，如果特征特别多而且只有个别有用那么选lasso，它们的区别就是正则化L1/L2。

## 线性分类



